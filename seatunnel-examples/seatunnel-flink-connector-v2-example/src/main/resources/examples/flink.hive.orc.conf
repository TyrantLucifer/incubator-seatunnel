env {
  # You can set spark configuration here
  # see available properties defined by spark: https://spark.apache.org/docs/latest/configuration.html#available-properties
  #job.mode = BATCH
  execution.parallelism = 1
  job.mode = "BATCH"
}

source {
  # This is a example input plugin **only for test and demonstrate the feature input plugin**
  Hive {
    file.path = "/apps/hive/warehouse/demo.db/rtp_elpoc_id1"
    fs.defaultFS = "hdfs://namenode14812"
    file.type = "orc"
  }
}

transform {
}

sink {
  # choose stdout output plugin to output data to console
  Console {}

  # you can also you other output plugins, such as sql
  # hdfs {
  #   path = "hdfs://hadoop-cluster-01/nginx/accesslog_processed"
  #   save_mode = "append"
  # }

  # If you would like to get more information about how to configure seatunnel and see full list of output plugins,
  # please go to https://seatunnel.apache.org/docs/spark/configuration/sink-plugins/Console
}
